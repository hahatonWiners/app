# Реализация алгоритма - главной части кейса.
## Резюме
После анализа целевой задачи и тех данных, что у нас есть, мы поняли, что написать с нуля свой собственный алгоритм - отчаянное дело. Наши данные можно было бы представить в виде графа, обучить на нём GNN модель, или же просто представить их в качестве таблицы и использовать банальную регрессию. Но важно одно - тип обучения в любом случае "без учителя". Графовые нейросети имеют в себе таковую возможность. Тем не менее, с ними очень трудно работать: начиная от представления данных и заканчивая признаковым пространством - везде поджидают трудности. Вне зависимости от выбора модели, придётся как-то её оценивать. По условиям кейса учитывается 4 критерия, так что всю задачу можно представить как задачу многокритериальной оптимизации - что мы и сделали, решив использовать **Генетические алгоритмы**

## Что по метрикам?
Определившись со способом решения мы подписали приговор на оптимизацию 4-х критериев:
- равномерное размещение контейнеров по поездам (разница ≤ m*)
- минимизировать сумму приоритетов
- минимизировать разницу суммарного веса между поездами
- в каждом поезде должно быть как можно больше однотипных контейнеров

  Если с первыми тремя всё понятно - мы, исходя из условия, просто минимизируем их.
  Для оценки последнего критерия мы решили использовать accuracy, делая это следующим образом:
  1). Для всех поездов в существующем решении определяем, какой тип контейнеров встречается на поезде чаще всего.
  2). Считаем, какую долю от общего числа контейнеров занимает этот тип контейнера.
  3). Для каждого контейнера получилось что-то вроде accuracy-score, поэтому мы берём их среднее значение.
  4). Так как алгоритм будет минимизировать все метрики, то домножаем результат на -1, обманывая систему. Чем меньше результат, тем в итоге лучше наш score

## Какой же конкретно алгоритм мы выбрали?
NSGA-II. Далее я расписал, почему именно его, и как в целом работают алгоритмы многокритериальной оптимизации.


# Многокритериальная оптимизация и алгоритм NSGA-II

## Введение

Многокритериальная оптимизация (Multi-Objective Optimization, MOO) — это раздел оптимизации, в котором одновременно решается задача улучшения нескольких, зачастую конфликтующих, критериев или целей. В отличие от классической одноцелевой оптимизации, где существует единственный оптимум, в многокритериальной оптимизации обычно ищется множество компромиссных решений, известных как **эффективный фронт Парето**.

### Основные понятия

- **Критерии (цели)** — функции, которые нужно оптимизировать (минимизировать или максимизировать).
- **Конфликтующие цели** — улучшение одного критерия может привести к ухудшению другого.
- **Парето-оптимальность** — решение называется Парето-оптимальным, если невозможно улучшить хотя бы один критерий без ухудшения другого.
- **Фронт Парето** — множество всех Парето-оптимальных решений, представляющее компромисс между критериями.

---

## Алгоритмы многокритериальной оптимизации

Для поиска фронта Парето разработаны различные методы, среди которых:

- **Классические методы**: взвешивание критериев, метод ограничения, метод ε-ограничений.
- **Эволюционные алгоритмы**: основаны на популяционном подходе и эволюционных операторах (мутация, скрещивание).
- **Методы на основе роя частиц, дифференциальной эволюции и др.**

Эволюционные алгоритмы особенно популярны, так как они способны эффективно находить множество решений за один запуск и хорошо работают с нелинейными, дискретными и сложными задачами.

---

## NSGA-II: Non-dominated Sorting Genetic Algorithm II

NSGA-II — один из самых известных и широко применяемых эволюционных алгоритмов для многокритериальной оптимизации, предложенный К. Дебом и соавторами в 2002 году. Он сочетает в себе эффективную сортировку по доминированию и механизм сохранения разнообразия решений.

### Основные особенности NSGA-II

- **Сортировка по уровню доминирования (Non-dominated Sorting):**  
  Популяция разбивается на несколько фронтов Парето, где первый фронт — наилучшие решения (не доминируемые никакими другими), второй — доминируемые только решениями первого фронта, и так далее.

- **Crowding Distance (расстояние плотности):**  
  Для сохранения разнообразия решений внутри фронта используется мера плотности соседних решений, что предотвращает сходимость к узкой области пространства решений.

- **Отбор и генерация потомков:**  
  Используются стандартные генетические операторы — селекция, кроссовер и мутация, с учётом рангов фронтов и расстояния плотности.

### Преимущества NSGA-II

- Эффективность и масштабируемость при работе с несколькими целями.
- Хорошее распределение решений по фронту Парето.
- Относительная простота реализации и широкое применение в инженерных и научных задачах.
