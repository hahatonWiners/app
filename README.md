# Реализация алгоритма - главной части кейса.
## Резюме
После анализа целевой задачи и тех данных, что у нас есть, мы поняли, что написать с нуля свой собственный алгоритм - отчаянное дело. Наши данные можно было бы представить в виде графа, обучить на нём GNN модель, или же просто представить их в качестве таблицы и использовать банальную регрессию. Но важно одно - тип обучения в любом случае "без учителя". Графовые нейросети имеют в себе таковую возможность. Тем не менее, с ними очень трудно работать: начиная от представления данных и заканчивая признаковым пространством - везде поджидают трудности. Вне зависимости от выбора модели, придётся как-то её оценивать. По условиям кейса учитывается 4 критерия, так что всю задачу можно представить как задачу многокритериальной оптимизации - что мы и сделали, решив использовать **Генетические алгоритмы**

## Что по метрикам?
Определившись со способом решения мы подписали приговор на оптимизацию 4-х критериев:
- равномерное размещение контейнеров по поездам (разница ≤ m*)
- минимизировать сумму приоритетов
- минимизировать разницу суммарного веса между поездами
- в каждом поезде должно быть как можно больше однотипных контейнеров

 
Если с первыми тремя критериями всё понятно — мы просто минимизируем их согласно условию задачи. Для оценки последнего критерия мы решили использовать метрику **accuracy**, реализованную следующим образом:
1. Для каждого поезда в текущем решении определяем тип контейнеров, который встречается на поезде **чаще всего**.
2. Вычисляем долю контейнеров этого типа от общего числа контейнеров на данном поезде.
3. Таким образом, для каждого контейнера получается нечто похожее на accuracy-score.
4. Для итоговой оценки берём среднее значение этих accuracy-score по всем контейнерам.
5. Поскольку алгоритм минимизирует все метрики, мы домножаем результат на **-1**, тем самым «обманывая» систему: чем меньше итоговое значение, тем лучше наш score.
  
## Какой же конкретно алгоритм мы выбрали?
NSGA-II. Далее я расписал, почему именно его, и как в целом работают алгоритмы многокритериальной оптимизации.
  
  
# Многокритериальная оптимизация и алгоритм NSGA-II

## Введение

Многокритериальная оптимизация (Multi-Objective Optimization, MOO) — это раздел оптимизации, в котором одновременно решается задача улучшения нескольких, зачастую конфликтующих, критериев или целей. В отличие от классической одноцелевой оптимизации, где существует единственный оптимум, в многокритериальной оптимизации обычно ищется множество компромиссных решений, известных как **эффективный фронт Парето**.

### Основные понятия

- **Критерии (цели)** — функции, которые нужно оптимизировать (минимизировать или максимизировать).
- **Конфликтующие цели** — улучшение одного критерия может привести к ухудшению другого.
- **Парето-оптимальность** — решение называется Парето-оптимальным, если невозможно улучшить хотя бы один критерий без ухудшения другого.
- **Фронт Парето** — множество всех Парето-оптимальных решений, представляющее компромисс между критериями.

---

## Алгоритмы многокритериальной оптимизации

Для поиска фронта Парето разработаны различные методы, среди которых:

- **Классические методы**: взвешивание критериев, метод ограничения, метод ε-ограничений.
- **Эволюционные алгоритмы**: основаны на популяционном подходе и эволюционных операторах (мутация, скрещивание).
- **Методы на основе роя частиц, дифференциальной эволюции и др.**

Эволюционные алгоритмы особенно популярны, так как они способны эффективно находить множество решений за один запуск и хорошо работают с нелинейными, дискретными и сложными задачами.

---

## NSGA-II: Non-dominated Sorting Genetic Algorithm II

NSGA-II — один из самых известных и широко применяемых эволюционных алгоритмов для многокритериальной оптимизации, предложенный К. Дебом и соавторами в 2002 году. Он сочетает в себе эффективную сортировку по доминированию и механизм сохранения разнообразия решений.

### Основные особенности NSGA-II

- **Сортировка по уровню доминирования (Non-dominated Sorting):**  
  Популяция разбивается на несколько фронтов Парето, где первый фронт — наилучшие решения (не доминируемые никакими другими), второй — доминируемые только решениями первого фронта, и так далее.

- **Crowding Distance (расстояние плотности):**  
  Для сохранения разнообразия решений внутри фронта используется мера плотности соседних решений, что предотвращает сходимость к узкой области пространства решений.

- **Отбор и генерация потомков:**  
  Используются стандартные генетические операторы — селекция, кроссовер и мутация, с учётом рангов фронтов и расстояния плотности.

### Преимущества NSGA-II

- Эффективность и масштабируемость при работе с несколькими целями.
- Хорошее распределение решений по фронту Парето.
- Относительная простота реализации и широкое применение в инженерных и научных задачах.
